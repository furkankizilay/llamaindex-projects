{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Lesson 4: Building a Multi-Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323703",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## 1. Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b71ff6",
   "metadata": {},
   "source": [
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff58c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2c6a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. Additionally, the models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, with promising results on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, with promising results on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that improves the quality and factuality of large language models by incorporating adaptive retrieval of passages, generating text based on retrieved information, and evaluating its own output using reflection tokens. This system outperforms existing models by enhancing generation quality, factuality, and verifiability through a combination of retrieval mechanisms and self-assessment techniques.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models with minimal accuracy compromise. It uses shifted sparse attention (S2-Attn) to approximate the standard self-attention pattern during training. This method retains the original attention architecture during inference, making it compatible with existing optimization and infrastructure. Additionally, LongLoRA bridges the gap between LoRA and full fine-tuning by allowing trainable normalization and embedding layers, enabling the extension of context lengths for models like Llama2 7B to 100k and 70B to 32k on a single 8× A100 machine.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating adaptive retrieval of passages, generating text based on retrieved information, and evaluating its own output using reflection tokens. It outperforms existing models by improving generation quality, factuality, and verifiability through retrieval mechanisms and self-assessment techniques.\n",
      "\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models with minimal accuracy compromise. It uses shifted sparse attention (S2-Attn) to approximate the standard self-attention pattern during training. LongLoRA retains the original attention architecture during inference, making it compatible with existing optimization and infrastructure. It bridges the gap between LoRA and full fine-tuning by allowing trainable normalization and embedding layers, enabling the extension of context lengths for models like Llama2 7B to 100k and 70B to 32k on a single 8× A100 machine.\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating adaptive retrieval of passages, generating text based on retrieved information, and evaluating its own output using reflection tokens. It outperforms existing models by improving generation quality, factuality, and verifiability through retrieval mechanisms and self-assessment techniques.\n",
      "\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models with minimal accuracy compromise. It uses shifted sparse attention (S2-Attn) to approximate the standard self-attention pattern during training. LongLoRA retains the original attention architecture during inference, making it compatible with existing optimization and infrastructure. It bridges the gap between LoRA and full fine-tuning by allowing trainable normalization and embedding layers, enabling the extension of context lengths for models like Llama2 7B to 100k and 70B to 32k on a single 8× A100 machine.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eede70c",
   "metadata": {},
   "source": [
    "## 2. Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771e69",
   "metadata": {},
   "source": [
    "### Download 11 ICLR papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    #\"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77426cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download these papers, below is the needed code:\n",
    "\n",
    "\n",
    "    #for url, paper in zip(urls, papers):\n",
    "         #!wget \"{url}\" -O \"{paper}\"\n",
    "    \n",
    "    \n",
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35d52c",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b03dfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in the MetaGPT project includes two public benchmarks, HumanEval and MBPP, along with a self-generated SoftwareDev dataset. The HumanEval benchmark consists of 164 handwritten programming tasks, while the MBPP benchmark comprises 427 Python tasks. The SoftwareDev dataset contains 70 representative examples of software development tasks, covering diverse scopes such as mini-games, image processing algorithms, and data visualization. These datasets were utilized to evaluate the performance of MetaGPT in code generation tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset comprises task instances extracted from a variety of open-source repositories, each presenting a specific problem statement along with corresponding codebase modifications. These task instances are utilized to gauge the performance of different models in creating patches to resolve the stated issues. The dataset includes details on the resolution status of each task instance, the complexity of the code changes, and the success rates of applying the generated patches. It serves as a critical tool for assessing the efficacy of language models in tackling real-world software engineering challenges.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes two public benchmarks, HumanEval and MBPP, along with a self-generated SoftwareDev dataset. The HumanEval benchmark consists of 164 handwritten programming tasks, while the MBPP benchmark comprises 427 Python tasks. The SoftwareDev dataset contains 70 representative examples of software development tasks, covering diverse scopes such as mini-games, image processing algorithms, and data visualization. These datasets were utilized to evaluate the performance of MetaGPT in code generation tasks.\n",
      "\n",
      "On the other hand, the evaluation dataset in SWE-Bench comprises task instances extracted from various open-source repositories, each presenting a specific problem statement along with corresponding codebase modifications. These task instances are used to assess the performance of different models in creating patches to resolve the stated issues. The dataset includes details on the resolution status of each task instance, the complexity of the code changes, and the success rates of applying the generated patches. It serves as a critical tool for evaluating the efficacy of language models in addressing real-world software engineering challenges.\n",
      "The evaluation dataset used in MetaGPT includes two public benchmarks, HumanEval and MBPP, along with a self-generated SoftwareDev dataset. The HumanEval benchmark consists of 164 handwritten programming tasks, while the MBPP benchmark comprises 427 Python tasks. The SoftwareDev dataset contains 70 representative examples of software development tasks, covering diverse scopes such as mini-games, image processing algorithms, and data visualization. These datasets were utilized to evaluate the performance of MetaGPT in code generation tasks.\n",
      "\n",
      "On the other hand, the evaluation dataset in SWE-Bench comprises task instances extracted from various open-source repositories, each presenting a specific problem statement along with corresponding codebase modifications. These task instances are used to assess the performance of different models in creating patches to resolve the stated issues. The dataset includes details on the resolution status of each task instance, the complexity of the code changes, and the success rates of applying the generated patches. It serves as a critical tool for evaluating the efficacy of language models in addressing real-world software engineering challenges.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"Analyzing the approach in the LongLoRA paper.\"}\n",
      "=== Function Output ===\n",
      "The approach in the LongLoRA paper introduces an efficient fine-tuning method that utilizes shifted sparse attention (S2-Attn) during training to approximate standard self-attention patterns. This technique extends the context window of large language models (LLMs) while reducing GPU memory cost and training time compared to full fine-tuning. By incorporating trainable normalization and embedding layers, LongLoRA bridges the gap between low-rank adaptation (LoRA) and full fine-tuning, enabling effective long context adaptation. The paper demonstrates promising results in extending the context length of Llama2 models and shows strong performance across various tasks. Additionally, the LongLoRA paper introduces an Action Units Relation Learning framework, consisting of the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) components, which achieve state-of-the-art performance in forgery detection and generalization to unseen manipulation methods. Visualizations of tampered regions using predicted patch tokens aid in understanding how different regions are modified during the manipulation process.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"Analyzing the approach in the LoftQ paper.\"}\n",
      "=== Function Output ===\n",
      "The LoftQ paper introduces a novel quantization framework that combines quantization and low-rank approximation to approximate high-precision pre-trained weights. This approach aims to provide a better initialization for subsequent LoRA fine-tuning, leading to improved performance on downstream tasks. LoftQ demonstrates effectiveness and robustness, especially in low-bit quantization regimes, outperforming existing methods like QLoRA. The alternating optimization used in LoftQ helps narrow the discrepancy between quantized weights and pre-trained weights, leading to enhanced fine-tuning performance. Additionally, LoftQ shows promising results in various tasks such as natural language understanding, question answering, summarization, and natural language generation. LoftQ focuses on quantization techniques for model compression, specifically targeting the DeBERTaV3-base model, aiming to achieve performance close to full finetuning while reducing memory requirements during both training and storage. By utilizing low-rank adapters, LoftQ can be applied to convolutional layers as well, enabling efficient compression of neural network models. LoftQ also outperforms state-of-the-art pruning methods in terms of model performance and memory savings, showcasing its effectiveness as a compression technique for deep learning models.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning method that utilizes shifted sparse attention (S2-Attn) during training to approximate standard self-attention patterns. This technique extends the context window of large language models (LLMs) while reducing GPU memory cost and training time compared to full fine-tuning. By incorporating trainable normalization and embedding layers, LongLoRA bridges the gap between low-rank adaptation (LoRA) and full fine-tuning, enabling effective long context adaptation. The paper demonstrates promising results in extending the context length of Llama2 models and shows strong performance across various tasks. Additionally, the LongLoRA paper introduces an Action Units Relation Learning framework, consisting of the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) components, which achieve state-of-the-art performance in forgery detection and generalization to unseen manipulation methods. Visualizations of tampered regions using predicted patch tokens aid in understanding how different regions are modified during the manipulation process.\n",
      "\n",
      "On the other hand, the LoftQ paper introduces a novel quantization framework that combines quantization and low-rank approximation to approximate high-precision pre-trained weights. This approach aims to provide a better initialization for subsequent LoRA fine-tuning, leading to improved performance on downstream tasks. LoftQ demonstrates effectiveness and robustness, especially in low-bit quantization regimes, outperforming existing methods like QLoRA. The alternating optimization used in LoftQ helps narrow the discrepancy between quantized weights and pre-trained weights, leading to enhanced fine-tuning performance. Additionally, LoftQ shows promising results in various tasks such as natural language understanding, question answering, summarization, and natural language generation. LoftQ focuses on quantization techniques for model compression, specifically targeting the DeBERTaV3-base model, aiming to achieve performance close to full finetuning while reducing memory requirements during both training and storage. By utilizing low-rank adapters, LoftQ can be applied to convolutional layers as well, enabling efficient compression of neural network models. LoftQ also outperforms state-of-the-art pruning methods in terms of model performance and memory savings, showcasing its effectiveness as a compression technique for deep learning models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d88c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
